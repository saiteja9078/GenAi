{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AsMFKCALr3y"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aurelio-labs/langchain-course/blob/main/chapters/07-lcel.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PQji83qLpVC"
      },
      "source": [
        "#### LangChain Essentials Course"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22dLBWVZLpVD"
      },
      "source": [
        "# LangChains Expression Language"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiRUqmfBLpVE"
      },
      "source": [
        "LangChain is one of the most popular open source libraries for AI Engineers. It's goal is to abstract away the complexity in building AI software, provide easy-to-use building blocks, and make it easier when switching between AI service providers.\n",
        "\n",
        "In this example, we will introduce LangChain's Expression Langauge (LCEL), abstracting a full chain and understanding how it will work. We'll provide examples for both OpenAI's `gpt-4o-mini` *and* Meta's `llama3.2` via Ollama!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "i690tIabLwb_",
        "outputId": "dde8e0cd-e76e-48b4-f89e-a7369d2531a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.26)\n",
            "Requirement already satisfied: google-search-results in /usr/local/lib/python3.11/dist-packages (2.4.2)\n",
            "Requirement already satisfied: langchain_core in /usr/local/lib/python3.11/dist-packages (0.3.66)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.5)\n",
            "Requirement already satisfied: langchain_google_genai in /usr/local/lib/python3.11/dist-packages (2.0.10)\n",
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.11/dist-packages (0.3.26)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.45)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (4.14.0)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.25.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.172.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from langchain_google_genai) (1.2.0)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.10.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain_core) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.6.15)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.73.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain google-search-results langchain_core google-generativeai langchain_google_genai langchain_community"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYGeByKjLpVF"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQbgErgdLpVF"
      },
      "source": [
        "## Traditional Chains vs LCEL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dyluns_ELpVF"
      },
      "source": [
        "In this section we're going to dive into a basic example using the traditional method for building chains before jumping into LCEL. We will build a pipeline where the user must input a specific topic, and then the LLM will look and return a report on the specified topic. Generating a _research report_ for the user."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTZTTiK7LpVF"
      },
      "source": [
        "### Traditional LLMChain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDHfbKZ7LpVF"
      },
      "source": [
        "The `LLMChain` is the simplest chain originally introduced in LangChain. This chain takes a prompt, feeds it into an LLM, and _optionally_ adds an output parsing step before returning the result.\n",
        "\n",
        "Let's see how we construct this using the traditional method, for this we need:\n",
        "\n",
        "* `prompt` — a `PromptTemplate` that will be used to generate the prompt for the LLM.\n",
        "* `llm` — the LLM we will be using to generate the output.\n",
        "* `output_parser` — an optional output parser that will be used to parse the structured output of the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Jyz2vWLoLpVF"
      },
      "outputs": [],
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "prompt_template = \"Give me a very small report on {topic}\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=prompt_template\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rykro39YLpVF"
      },
      "source": [
        "\n",
        "\n",
        "We will use the `gemini-2.0-flash` model with a `temperature` of `0.0`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Uvs7hILRLpVF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    temperature=0.0,\n",
        "    api_key=\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhs6_3c4LpVF",
        "outputId": "07bec0a4-ceb3-4b06-d914-5caf648d1637"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hello! How can I help you today?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--7c2e78c8-9dc7-47ae-8e01-45b135c7c426-0', usage_metadata={'input_tokens': 2, 'output_tokens': 10, 'total_tokens': 12, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "llm_out = llm.invoke(\"Hello there\")\n",
        "llm_out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V39qjMpLLpVG"
      },
      "source": [
        "Then we define our output parser, this will be used to parse the output of the LLM. In this case, we will use the `StrOutputParser` which will parse the `AIMessage` output from our LLM into a single string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "46qTjklnLpVG"
      },
      "outputs": [],
      "source": [
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "output_parser = StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "z0uwxedBLpVG",
        "outputId": "5fe2e346-3591-438b-feb4-ef9240ef9838"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello! How can I help you today?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "out = output_parser.invoke(llm_out)\n",
        "out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mk4CUy52LpVG"
      },
      "source": [
        "Through the `LLMChain` class we can place each of our components into a linear `chain`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "PrFk1J9dLpVG",
        "outputId": "fc5dc02c-490e-4cb2-874a-6d19e22aa5d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-10-2960353250.py:3: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  chain = LLMChain(prompt=prompt, llm=llm, output_parser=output_parser)\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import LLMChain\n",
        "\n",
        "chain = LLMChain(prompt=prompt, llm=llm, output_parser=output_parser)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjNsVMVpLpVG"
      },
      "source": [
        "Note that the `LLMChain` _was_ deprecated in LangChain `0.1.17`, the expected way of constructing these chains today is through LCEL, which we'll cover in a moment.\n",
        "\n",
        "We can `invoke` our `chain`, providing a `topic` that we'd like to be researched."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDDq8Cq7LpVG",
        "outputId": "4afda0a7-a5fd-4e9e-c296-0be404fca54a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'topic': 'retrieval augmented generation',\n",
              " 'text': '## Retrieval Augmented Generation (RAG): A Quick Look\\n\\nRetrieval Augmented Generation (RAG) is a technique that enhances the capabilities of large language models (LLMs) by allowing them to access and incorporate external knowledge sources during the generation process.\\n\\n**How it works:**\\n\\n1.  **Retrieval:** Given a user query, RAG first retrieves relevant information from a knowledge base (e.g., documents, databases, web pages).\\n2.  **Augmentation:** The retrieved information is then combined with the original query.\\n3.  **Generation:** The LLM uses this augmented prompt to generate a more informed and contextually relevant response.\\n\\n**Benefits:**\\n\\n*   **Improved Accuracy:** Reduces hallucinations and provides more factual answers.\\n*   **Access to Up-to-Date Information:** Allows LLMs to leverage current knowledge beyond their training data.\\n*   **Enhanced Contextual Understanding:** Provides richer context for generating more relevant and nuanced responses.\\n\\n**In short, RAG empowers LLMs to be more knowledgeable and reliable by grounding their responses in external, verifiable information.**'}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "result = chain.invoke(\"retrieval augmented generation\")\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbHqc1qLLpVG"
      },
      "source": [
        "We can view a formatted version of this output using the `Markdown` display:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "id": "hBxPHOZ-LpVG",
        "outputId": "6a81a135-2df4-4f79-b652-097d7d410dcc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Retrieval Augmented Generation (RAG): A Quick Look\n\nRetrieval Augmented Generation (RAG) is a technique that enhances the capabilities of large language models (LLMs) by allowing them to access and incorporate external knowledge sources during the generation process.\n\n**How it works:**\n\n1.  **Retrieval:** Given a user query, RAG first retrieves relevant information from a knowledge base (e.g., documents, databases, web pages).\n2.  **Augmentation:** The retrieved information is then combined with the original query.\n3.  **Generation:** The LLM uses this augmented prompt to generate a more informed and contextually relevant response.\n\n**Benefits:**\n\n*   **Improved Accuracy:** Reduces hallucinations and provides more factual answers.\n*   **Access to Up-to-Date Information:** Allows LLMs to leverage current knowledge beyond their training data.\n*   **Enhanced Contextual Understanding:** Provides richer context for generating more relevant and nuanced responses.\n\n**In short, RAG empowers LLMs to be more knowledgeable and reliable by grounding their responses in external, verifiable information.**"
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "display(Markdown(result[\"text\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cbmq0DfeLpVG"
      },
      "source": [
        "That is a simple `LLMChain` using the traditional LangChain method. Now let's move onto LCEL."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWets8OpLpVG"
      },
      "source": [
        "## LangChain Expression Language (LCEL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Lr5NA0xLpVG"
      },
      "source": [
        "**L**ang**C**hain **E**xpression **L**anguage (LCEL) is the recommended approach to building chains in LangChain. Having superceeded the traditional methods with `LLMChain`, etc. LCEL gives us a more flexible system for building chains. The pipe operator `|` is used by LCEL to _chain_ together components. Let's see how we'd construct an `LLMChain` using LCEL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "8R-9b7ulLpVH"
      },
      "outputs": [],
      "source": [
        "lcel_chain = prompt | llm | output_parser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdJvzetzLpVH"
      },
      "source": [
        "We can `invoke` this chain in the same way as we did before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "XKWzIwpCLpVH",
        "outputId": "9a8175bc-1244-4b9f-e7c6-4e0a63be3549"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'## Retrieval Augmented Generation (RAG): A Quick Look\\n\\nRetrieval Augmented Generation (RAG) is a technique that enhances the capabilities of large language models (LLMs) by allowing them to access and incorporate external knowledge sources during the generation process.\\n\\n**How it works:**\\n\\n1.  **Retrieval:** Given a user query, RAG first retrieves relevant information from a knowledge base (e.g., documents, databases, web pages).\\n2.  **Augmentation:** The retrieved information is then combined with the original query.\\n3.  **Generation:** The LLM uses this augmented prompt to generate a more informed and contextually relevant response.\\n\\n**Benefits:**\\n\\n*   **Improved Accuracy:** Reduces hallucinations and provides more factual answers.\\n*   **Access to Up-to-Date Information:** Allows LLMs to leverage current knowledge beyond their training data.\\n*   **Enhanced Contextual Understanding:** Provides richer context for generating more relevant and nuanced responses.\\n\\n**In short, RAG empowers LLMs to be more knowledgeable and reliable by grounding their responses in external, verifiable information.**'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "result = lcel_chain.invoke(\"retrieval augmented generation\")\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7E8_msHLpVH"
      },
      "source": [
        "The output format is slightly different, but the underlying functionality and content being output is the same. As before, we can view a formatted version of this output using the `Markdown` display:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "id": "42uPzB1JLpVH",
        "outputId": "fc690597-2b77-4dd9-8553-a4fd6dc53b60"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Retrieval Augmented Generation (RAG): A Quick Look\n\nRetrieval Augmented Generation (RAG) is a technique that enhances the capabilities of large language models (LLMs) by allowing them to access and incorporate external knowledge sources during the generation process.\n\n**How it works:**\n\n1.  **Retrieval:** Given a user query, RAG first retrieves relevant information from a knowledge base (e.g., documents, databases, web pages).\n2.  **Augmentation:** The retrieved information is then combined with the original query.\n3.  **Generation:** The LLM uses this augmented prompt to generate a more informed and contextually relevant response.\n\n**Benefits:**\n\n*   **Improved Accuracy:** Reduces hallucinations and provides more factual answers.\n*   **Access to Up-to-Date Information:** Allows LLMs to leverage current knowledge beyond their training data.\n*   **Enhanced Contextual Understanding:** Provides richer context for generating more relevant and nuanced responses.\n\n**In short, RAG empowers LLMs to be more knowledgeable and reliable by grounding their responses in external, verifiable information.**"
          },
          "metadata": {}
        }
      ],
      "source": [
        "display(Markdown(result))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbutFcpyLpVI"
      },
      "source": [
        "### How Does the Pipe Operator Work?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIjOY7VILpVI"
      },
      "source": [
        "Before moving onto other LCEL features, let's take a moment to understand what the pipe operator `|` is doing and _how_ it works.\n",
        "\n",
        "Functionality wise, the pipe tells you that whatever the _left_ side outputs will be fed as input into the _right_ side. In the example of `prompt | llm | output_parser`, we see that `prompt` feeds into `llm` feeds into `output_parser`.\n",
        "\n",
        "The pipe operator is a way of chaining together components, and is a way of saying that whatever the _left_ side outputs will be fed as input into the _right_ side.\n",
        "\n",
        "Let's make a basic class named `Runnable` that will transform our a provided function into a _runnable_ class that we will then use with the pipe `|` operator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "9xTvwvF4LpVI"
      },
      "outputs": [],
      "source": [
        "class Runnable:\n",
        "    def __init__(self, func):\n",
        "        self.func = func\n",
        "    def __or__(self, other):\n",
        "        def chained_func(*args, **kwargs):\n",
        "            return other.invoke(self.func(*args, **kwargs))\n",
        "        return Runnable(chained_func)\n",
        "    def invoke(self, *args, **kwargs):\n",
        "        return self.func(*args, **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2Vg_PhXLpVI"
      },
      "source": [
        "With the `Runnable` class, we will be able wrap a function into the class, allowing us to then chain together multiple of these _runnable_ functions using the `__or__` method.\n",
        "\n",
        "First, let's create a few functions that we'll chain together:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "G7HCw9-VLpVI"
      },
      "outputs": [],
      "source": [
        "def add_five(x):\n",
        "    return x+5\n",
        "\n",
        "def sub_five(x):\n",
        "    return x-5\n",
        "\n",
        "def mul_five(x):\n",
        "    return x*5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0fDvmtWLpVI"
      },
      "source": [
        "Now we wrap our functions with the `Runnable`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "LfnBBcbxLpVI"
      },
      "outputs": [],
      "source": [
        "add_five_runnable = Runnable(add_five)\n",
        "sub_five_runnable = Runnable(sub_five)\n",
        "mul_five_runnable = Runnable(mul_five)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jHLbl_7LpVI"
      },
      "source": [
        "Finally, we can chain these together using the `__or__` method from the `Runnable` class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHFo61TwLpVI",
        "outputId": "1e06687d-bd9a-4fd9-de63-17506d11435a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "chain = (add_five_runnable).__or__(sub_five_runnable).__or__(mul_five_runnable)\n",
        "\n",
        "chain.invoke(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oim1zWHLpVI"
      },
      "source": [
        "So we can see that we're able to chain together our functions using `__or__`. The pipe `|` operator is simply a shortcut for the `__or__` method, so we can create the exact same chain like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uh3-jF8pLpVJ",
        "outputId": "c859cb47-d3fc-42a4-b561-7dd456e0116a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "chain = add_five_runnable | sub_five_runnable | mul_five_runnable\n",
        "\n",
        "chain.invoke(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Lnj00T8LpVJ"
      },
      "source": [
        "## LCEL `RunnableLambda`\n",
        "\n",
        "The `RunnableLambda` class is LangChain's built-in method for constructing a _runnable_ object from a function. That is, it does the same thing as the custom `Runnable` class we created earlier. Let's try it out with the same functions as before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "BmMH8GzVLpVJ"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "add_five_runnable = RunnableLambda(add_five)\n",
        "sub_five_runnable = RunnableLambda(sub_five)\n",
        "mul_five_runnable = RunnableLambda(mul_five)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkpwIKq6LpVJ"
      },
      "source": [
        "We chain these together again with the pipe `|` operator:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "9e58vaSELpVJ"
      },
      "outputs": [],
      "source": [
        "chain = add_five_runnable | sub_five_runnable | mul_five_runnable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FdqbdAyLpVJ"
      },
      "source": [
        "And call them using the `invoke` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqHBMT8ULpVJ",
        "outputId": "2cf5385d-e73a-45a2-c71c-58deba7fc4b9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "chain.invoke(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47mu4xxqLpVJ"
      },
      "source": [
        "Now we want to try something a little more testing, so this time we will generate a report, and we will try and edit that report using this functionallity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "xRJLzshqLpVJ"
      },
      "outputs": [],
      "source": [
        "prompt_str = \"give me a very small report about {topic}\"\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=prompt_str\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "lfl_s4VALpVJ"
      },
      "outputs": [],
      "source": [
        "chain = prompt | llm | output_parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "WSMlRM8wLpVJ",
        "outputId": "6dce316a-4f4f-4336-d8b4-76240449b538"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## AI: A Quick Look\n\nArtificial Intelligence (AI) is rapidly evolving, encompassing technologies that enable machines to perform tasks typically requiring human intelligence. From simple automation to complex problem-solving, AI is impacting various sectors like healthcare, finance, and transportation. While offering immense potential for innovation and efficiency, ethical considerations surrounding bias, job displacement, and security remain crucial areas of focus. The future of AI hinges on responsible development and deployment."
          },
          "metadata": {}
        }
      ],
      "source": [
        "result = chain.invoke(\"AI\")\n",
        "display(Markdown(result))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XE7HOxwaLpVJ"
      },
      "source": [
        "Here we are making two functions, `extract_fact` to pull out the main content of our text and `replace_word` that will replace AI with Skynet!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Wc7ws8FkLpVJ"
      },
      "outputs": [],
      "source": [
        "def extract_fact(x):\n",
        "    if \"\\n\\n\" in x:\n",
        "        return \"\\n\".join(x.split(\"\\n\\n\")[1:])\n",
        "    else:\n",
        "        return x\n",
        "\n",
        "old_word = \"AI\"\n",
        "new_word = \"skynet\"\n",
        "\n",
        "def replace_word(x):\n",
        "    return x.replace(old_word, new_word)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktHrg5iSLpVK"
      },
      "source": [
        "Lets wrap these functions and see what the output is!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "_vFeqMY-LpVK"
      },
      "outputs": [],
      "source": [
        "extract_fact_runnable = RunnableLambda(extract_fact)\n",
        "replace_word_runnable = RunnableLambda(replace_word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "TDWN-tQzLpVK"
      },
      "outputs": [],
      "source": [
        "chain = prompt | llm | output_parser | extract_fact_runnable | replace_word_runnable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "OCODP8vLLpVK",
        "outputId": "2cee6701-1a1e-4d35-cb4f-61f7a0f79e89"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Retrieval Augmented Generation (RAG) is a technique that enhances the capabilities of large language models (LLMs) by allowing them to access and incorporate external knowledge sources. Instead of relying solely on their pre-trained knowledge, RAG models first **retrieve** relevant information from a database or document store based on the user's query. This retrieved information is then **augmented** with the original query and fed into the LLM to **generate** a more informed and accurate response.\n**Benefits:**\n*   **Improved Accuracy:** Access to up-to-date information reduces reliance on potentially outdated or incorrect pre-trained knowledge.\n*   **Reduced Hallucinations:** Grounding responses in external sources minimizes the risk of the LLM generating fabricated information.\n*   **Increased Transparency:** Users can often trace the source of the information used in the response, enhancing trust and understanding.\nIn essence, RAG empowers LLMs to be more knowledgeable, reliable, and transparent by leveraging the power of external knowledge retrieval."
          },
          "metadata": {}
        }
      ],
      "source": [
        "result = chain.invoke(\"retrieval augmented generation\")\n",
        "display(Markdown(result))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}