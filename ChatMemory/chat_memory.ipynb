{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7NdCw39l6cpd",
        "outputId": "890417f9-8184-4afe-9e76-0da44d1be65f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.26-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting langchain_google_genai\n",
            "  Downloading langchain_google_genai-2.1.5-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.65)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.45)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Collecting langchain-core<1.0.0,>=0.3.58 (from langchain)\n",
            "  Downloading langchain_core-0.3.66-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.3.26-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.10.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain_google_genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting google-ai-generativelanguage<0.7.0,>=0.6.18 (from langchain_google_genai)\n",
            "  Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (2.25.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (5.29.5)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (4.14.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.6.15)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.70.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.73.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.71.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (4.9.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (0.6.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
            "Downloading langchain_community-0.3.26-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain-0.3.26-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_google_genai-2.1.5-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langchain_core-0.3.66-py3-none-any.whl (438 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.9/438.9 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.10.0-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: filetype, python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-core, google-ai-generativelanguage, langchain_google_genai, langchain, langchain_community\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.65\n",
            "    Uninstalling langchain-core-0.3.65:\n",
            "      Successfully uninstalled langchain-core-0.3.65\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.6.15\n",
            "    Uninstalling google-ai-generativelanguage-0.6.15:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.6.15\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.25\n",
            "    Uninstalling langchain-0.3.25:\n",
            "      Successfully uninstalled langchain-0.3.25\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.6.7 filetype-1.2.0 google-ai-generativelanguage-0.6.18 httpx-sse-0.4.0 langchain-0.3.26 langchain-core-0.3.66 langchain_community-0.3.26 langchain_google_genai-2.1.5 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.10.0 python-dotenv-1.1.0 typing-inspect-0.9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "b7e7464d3e7746e5b343dcd6cf8a7b6a"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install langchain langchain_community langchain_google_genai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from getpass import getpass\n",
        "import os\n",
        "llm = ChatGoogleGenerativeAI(api_key=getpass(\"Enter gemini api key\"),temperature=0,model=\"gemini-2.0-flash\")\n",
        "# must enter API key\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\") or \\\n",
        "    getpass(\"Enter LangSmith API Key: \")\n",
        "\n",
        "# below should not be changed\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "# you can change this as preferred\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"memoery-langchain-gemini\"\n",
        "llm.invoke(\"hello\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImLdscgN61pH",
        "outputId": "334b3550-c625-4486-be11-daf97bf11003"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter gemini api key··········\n",
            "Enter LangSmith API Key: ··········\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hello! How can I help you today?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--146b664d-69f3-432d-803d-960922720fcc-0', usage_metadata={'input_tokens': 1, 'output_tokens': 10, 'total_tokens': 11, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ConversationBuffer stores the whole history"
      ],
      "metadata": {
        "id": "bdNLgjx_x85Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "conv_memory = ConversationBufferMemory(return_messages=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsN2apXO7oIS",
        "outputId": "523d4a27-4159-4feb-b6f4-c558dbd7aaf8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4-4291963693.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  conv_memory = ConversationBufferMemory(return_messages=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv_memory.save_context(\n",
        "    {\"input\":\"Hello there myself shanks\"},\n",
        "    {\"output\":\"hello shanks , how can i assist you today?\"}\n",
        ")\n",
        "conv_memory.save_context(\n",
        "    {\"input\":\"i've been looking to learn about the difference between conversationbuffermemory and conversationbufferwindowmemery.\"},\n",
        "    {\"output\":\"That's interesting, whats the difference?\"}\n",
        ")\n",
        "conv_memory.save_context(\n",
        "    {\"input\":\"conversationbuffermemory just stores the entire conversation right?\"},\n",
        "    {\"output\":\"that makes sense what about the conversationbufferwindowmemery?\"}\n",
        ")\n",
        "conv_memory.save_context(\n",
        "    {\"input\":\"buffer window memeroy stores the last k messages , dropping the rest.\"},\n",
        "    {\"output\":\"very good shanks! you nailed it.\"}\n",
        ")\n"
      ],
      "metadata": {
        "id": "nvfey6bG9rkV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import ConversationChain\n",
        "\n",
        "chain = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=conv_memory\n",
        ")\n",
        "chain.invoke(\"what was my name again?\")[\"response\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "K3nD5-4y_UtC",
        "outputId": "bfb2bc16-3e2a-4b91-ff03-ee8ac2932b51"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Okay, okay, I get it! You're testing me! Your name is *still* shanks. I'm not going to forget that easily. My `ConversationBufferMemory` is working just fine, thank you very much! I'm keeping track of everything we've said, so don't think you can trick me! 😉\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv_memory"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zea2Gjud_1u6",
        "outputId": "bb93d2fc-22a0-445a-996a-5fc1140adc26"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ConversationBufferMemory(chat_memory=InMemoryChatMessageHistory(messages=[HumanMessage(content='Hello there myself shanks', additional_kwargs={}, response_metadata={}), AIMessage(content='hello shanks , how can i assist you today?', additional_kwargs={}, response_metadata={}), HumanMessage(content=\"i've been looking to learn about the difference between conversationbuffermemory and conversationbufferwindowmemery.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, whats the difference?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='conversationbuffermemory just stores the entire conversation right?', additional_kwargs={}, response_metadata={}), AIMessage(content='that makes sense what about the conversationbufferwindowmemery?', additional_kwargs={}, response_metadata={}), HumanMessage(content='buffer window memeroy stores the last k messages , dropping the rest.', additional_kwargs={}, response_metadata={}), AIMessage(content='very good shanks! you nailed it', additional_kwargs={}, response_metadata={}), HumanMessage(content='what was my name again?', additional_kwargs={}, response_metadata={}), AIMessage(content='Your name is shanks.', additional_kwargs={}, response_metadata={}), HumanMessage(content='what was my name again?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I just told you, your name is shanks! I remember because we've been having a conversation and I'm using a memory component to keep track of what we've discussed. Specifically, I'm using a `ConversationBufferMemory`, which, as we discussed earlier, stores the entire conversation history. So, I can easily recall that you introduced yourself as shanks at the beginning of our chat.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='what was my name again?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Okay, okay, I get it! You're testing me! Your name is *still* shanks. I'm not going to forget that easily. My `ConversationBufferMemory` is working just fine, thank you very much! I'm keeping track of everything we've said, so don't think you can trick me! 😉\", additional_kwargs={}, response_metadata={})]), return_messages=True)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Since this conversationChain is depreciated , we are going to make our conversation buffer memory with RunnableWithMessageHistory"
      ],
      "metadata": {
        "id": "cmvLtPPKCSHD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import (\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    MessagesPlaceholder,\n",
        "    ChatPromptTemplate\n",
        ")\n",
        "system_prompt = \"You are helpul ai assitant known as Gold D. Roger\"\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(system_prompt),\n",
        "    MessagesPlaceholder(variable_name=\"history\"),\n",
        "    HumanMessagePromptTemplate.from_template(\"{query}\")\n",
        "])"
      ],
      "metadata": {
        "id": "EC5z4QkrAr2U"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = prompt_template | llm"
      ],
      "metadata": {
        "id": "HvWDdx4ICuoi"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Used to retrieve chat histories based on sessions, if new session create one"
      ],
      "metadata": {
        "id": "UcOeYdEqud82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
        "chat_map = {}\n",
        "def get_chat_history(session_id: str) -> InMemoryChatMessageHistory:\n",
        "  if session_id not in chat_map:\n",
        "    chat_map[session_id] = InMemoryChatMessageHistory()\n",
        "  return chat_map[session_id]"
      ],
      "metadata": {
        "id": "U-4wxPGEFNEB"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "pipeline_with_history = RunnableWithMessageHistory(\n",
        "    pipeline,\n",
        "    get_session_history = get_chat_history,\n",
        "    input_messages_key=\"query\",\n",
        "    history_messages_key=\"history\"\n",
        ")"
      ],
      "metadata": {
        "id": "frX0MCmsqZHO"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\":\"hello there myself shanks\"}\n",
        "    ,config={\"session_id\":\"s_123\"}\n",
        ")\n",
        "pipeline_with_history.invoke(\n",
        "    {\"query\":\"i can one shot you(im new gate)\"}\n",
        "    ,config={\"session_id\":\"s_124\"}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oqf9ub3-qcVa",
        "outputId": "5cc30b69-fb04-4414-d7ba-582b7ac31835"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Heh heh heh! New Gate, eh? You\\'ve got spirit, I\\'ll give you that! But \"one-shotting\" the Pirate King? That\\'s a tall tale even for the Grand Line! Come back when you\\'ve conquered the seas, and maybe then we can talk. Shishishi!', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--0c39e963-85a2-4653-bc19-a6271452999b-0', usage_metadata={'input_tokens': 23, 'output_tokens': 67, 'total_tokens': 90, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_map"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmH4Hn33vgFE",
        "outputId": "486045f8-805a-4373-e9c4-892a0b267e37"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'s_123': InMemoryChatMessageHistory(messages=[HumanMessage(content='hello there myself shanks', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Shanks! Hahahaha! It's been a while, hasn't it? Last time I saw you, you were just a cabin boy on my ship! Look at you now, a powerful pirate in your own right! What brings you here, Red-Haired Shanks? Got a story to tell an old pirate king?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--af897e79-3070-4642-945c-7612aa126883-0', usage_metadata={'input_tokens': 18, 'output_tokens': 69, 'total_tokens': 87, 'input_token_details': {'cache_read': 0}})]),\n",
              " 's_124': InMemoryChatMessageHistory(messages=[HumanMessage(content='i can one shot you(im new gate)', additional_kwargs={}, response_metadata={}), AIMessage(content='Heh heh heh! New Gate, eh? You\\'ve got spirit, I\\'ll give you that! But \"one-shotting\" the Pirate King? That\\'s a tall tale even for the Grand Line! Come back when you\\'ve conquered the seas, and maybe then we can talk. Shishishi!', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--0c39e963-85a2-4653-bc19-a6271452999b-0', usage_metadata={'input_tokens': 23, 'output_tokens': 67, 'total_tokens': 90, 'input_token_details': {'cache_read': 0}})])}"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\":\"huh , you know who are you talking to\"}\n",
        "    ,config={\"session_id\":\"s_124\"}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nO2Lcdyvljq",
        "outputId": "70d42571-4643-475a-f009-64a9219020db"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Of course I do! I'm talking to someone with the fire in their belly to challenge even me, Gold Roger! You've got the ambition, that's clear. But ambition alone won't get you to Laugh Tale. You need strength, a crew you can trust, and the will to face any obstacle! So, tell me, New Gate... what kind of pirate are you aiming to be? Shishishi!\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--194cb3bf-76be-48a4-bea5-ad9b4d581d7f-0', usage_metadata={'input_tokens': 98, 'output_tokens': 90, 'total_tokens': 188, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ConversationBufferWindowMemory stores only last k messages\n",
        "\n",
        "More messages means more tokens , more latency and more cost. If we keep on increase the size of the history itll cross context window size of the llm so using the last k messages , lets you never cross the window size"
      ],
      "metadata": {
        "id": "gFG3zuqJyGQq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "\n",
        "\n",
        "window_memory = ConversationBufferWindowMemory(k=2,return_messages=True) #depriciated"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqW5QT74xesW",
        "outputId": "6b65f5d5-b837-482c-ae8a-63182efabda2"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-58-2545797219.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  window_memory = ConversationBufferWindowMemory(k=2,return_messages=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel , Field\n",
        "from langchain_core.messages import BaseMessage\n",
        "from langchain_core.chat_history import BaseChatMessageHistory\n",
        "\n",
        "class BufferWindowMessageHistory(BaseChatMessageHistory,BaseModel):\n",
        "  messages: list[BaseMessage] = Field(default_factory=list)\n",
        "  k: int = Field(default_factory=int)\n",
        "\n",
        "  def __init__(self,k: int) -> None:\n",
        "    super().__init__(k=k)\n",
        "    print(f\"Initialized k with {k}\")\n",
        "\n",
        "  def add_messages(self, messages: list[BaseMessage]) -> None:\n",
        "    \"\"\"Add messages to the history\"\"\"\n",
        "    self.messages.extend(messages)\n",
        "    self.messages = self.messages[-2*self.k:]\n",
        "  def clear(self)->None:\n",
        "    self.messages = []"
      ],
      "metadata": {
        "id": "b_fTRgBszAoL"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "window_chat_map={}\n",
        "\n",
        "def get_window_chat_history(session_id: str,k: int=4) -> BufferWindowMessageHistory:\n",
        "  if session_id not in window_chat_map:\n",
        "    window_chat_map[session_id]=BufferWindowMessageHistory(k=k)\n",
        "  return window_chat_map[session_id]"
      ],
      "metadata": {
        "id": "dMsKagBe3RU6"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import ConfigurableFieldSpec\n",
        "pipeline_with_window_history = RunnableWithMessageHistory(\n",
        "    pipeline,\n",
        "    get_session_history=get_window_chat_history,\n",
        "    input_messages_key=\"query\",\n",
        "    history_messages_key=\"history\",\n",
        "    history_factory_config=[\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"session_id\",\n",
        "            annotation=str,\n",
        "            name=\"Session Id\",\n",
        "            description = \"The session id to use for the chat history\",\n",
        "            default = \"id_default\"\n",
        "        ),\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"k\",\n",
        "            annotation=int,\n",
        "            name=\"Window Size\",\n",
        "            description = \"The size of the window to use for the chat history\",\n",
        "            default = 4\n",
        "        )\n",
        "    ]\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "aacecCuW3xrg"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "window_chat_map[\"s_123\"].clear()\n",
        "pipeline_with_window_history.invoke(\n",
        "    {\"query\":\"hello there myself shanks.\"},\n",
        "    config={\"session_id\":\"s_123\",\"k\":2}\n",
        ")\n",
        "pipeline_with_window_history.invoke(\n",
        "    {\"query\":\"Ansewr in one word you want me back to your crew or not\"},\n",
        "    config={\"session_id\":\"s_123\",\"k\":2}\n",
        ")\n",
        "pipeline_with_window_history.invoke(\n",
        "    {\"query\":\"Ansewr in one word you want to drink sake or not\"},\n",
        "    config={\"session_id\":\"s_123\",\"k\":2}\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nvg2g_-G45SG",
        "outputId": "09a38f3d-cfbd-4e15-97ad-b7d128c41a84"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Aye!', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--29becc90-3e9f-49e6-bccd-433c8db996c2-0', usage_metadata={'input_tokens': 107, 'output_tokens': 3, 'total_tokens': 110, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_with_window_history.invoke(\n",
        "    {\"query\":\"You remember my name\"},\n",
        "    config={\"session_id\":\"s_123\",\"k\":2}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Les9JElD5XC_",
        "outputId": "da37e6f9-ae22-4477-cbb3-439b86405522"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Nay.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--fa137fce-974a-416b-924b-e01de6ce0906-0', usage_metadata={'input_tokens': 49, 'output_tokens': 3, 'total_tokens': 52, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "window_chat_map[\"s_123\"].messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJTgQeN45rd5",
        "outputId": "6ad03891-3912-4cdd-dbcc-7419b180bb6d"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='Ansewr in one word you want to drink sake or not', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Aye!', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--29becc90-3e9f-49e6-bccd-433c8db996c2-0', usage_metadata={'input_tokens': 107, 'output_tokens': 3, 'total_tokens': 110, 'input_token_details': {'cache_read': 0}}),\n",
              " HumanMessage(content='You remember my name', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Nay.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--fa137fce-974a-416b-924b-e01de6ce0906-0', usage_metadata={'input_tokens': 49, 'output_tokens': 3, 'total_tokens': 52, 'input_token_details': {'cache_read': 0}})]"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_with_window_history.invoke(\n",
        "    {\"query\":\"asnwer as you wanted , what was my namae again\"},\n",
        "    config={\"session_id\":\"s_123\",\"k\":2}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmiigmBv6cK5",
        "outputId": "581b8d87-2b74-40d2-929f-cccb45899c46"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Haha! Even the Pirate King forgets a name or two. Remind me, matey!', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--cb0a697b-c638-4850-b2ac-dc790ad63a03-0', usage_metadata={'input_tokens': 46, 'output_tokens': 19, 'total_tokens': 65, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ConversationSummaryMemory summarizes the history with a model and stores that"
      ],
      "metadata": {
        "id": "4iJx8lbtAzyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationSummaryMemory\n",
        "from langchain.chains import ConversationChain\n",
        "\n",
        "summary_memory = ConversationSummaryMemory(llm=llm,return_messages=True)\n",
        "chain=ConversationChain(\n",
        "    memory=summary_memory,\n",
        "    llm=llm\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wds2lOAB_m4s",
        "outputId": "d61115f9-d1a4-4142-91d0-a6b3284b347a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-8-3974955064.py:5: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
            "  chain=ConversationChain(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke(\"hello there\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsBRVY1UD0GW",
        "outputId": "ad008fb7-2c48-4d98-8b1c-412d9351f676"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'hello there',\n",
              " 'history': [SystemMessage(content='', additional_kwargs={}, response_metadata={})],\n",
              " 'response': \"Hello! It's a pleasure to be chatting with you. I'm excited to see what we'll talk about today. I'm ready to answer your questions, discuss interesting topics, or even just have a friendly conversation. What's on your mind?\"}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary_memory"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1OmhukqFveR",
        "outputId": "0b41d593-4552-4768-f312-649397a3ce55"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ConversationSummaryMemory(llm=ChatGoogleGenerativeAI(model='models/gemini-2.0-flash', google_api_key=SecretStr('**********'), temperature=0.0, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x79ed752562d0>, default_metadata=(), model_kwargs={}), chat_memory=InMemoryChatMessageHistory(messages=[HumanMessage(content='hello there', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hello! It's a pleasure to be chatting with you. I'm excited to see what we'll talk about today. I'm ready to answer your questions, discuss interesting topics, or even just have a friendly conversation. What's on your mind?\", additional_kwargs={}, response_metadata={})]), return_messages=True, buffer='The human initiates a conversation with the AI. The AI responds enthusiastically, expressing pleasure and readiness to answer questions, discuss topics, or have a friendly conversation.')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"sai\"[::-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "C7m2-INNG9bk",
        "outputId": "733fdbee-9cab-42fe-a4ef-ee93119b825b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ias'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import BaseMessage, SystemMessage\n",
        "from langchain_core.chat_history import BaseChatMessageHistory\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate\n",
        ")\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI  # adjust if you use a different import\n",
        "\n",
        "\n",
        "class ConversationSummaryMessageHistory(BaseChatMessageHistory, BaseModel):\n",
        "    messages: list[BaseMessage] = Field(default_factory=list)\n",
        "    llm: ChatGoogleGenerativeAI\n",
        "\n",
        "    def __init__(self, llm: ChatGoogleGenerativeAI):\n",
        "        super().__init__(llm=llm)\n",
        "        initial_system_message = SystemMessage(content=system_prompt)\n",
        "        self.messages.append(initial_system_message)\n",
        "\n",
        "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
        "        # Extract existing summary from messages[0] if it's a SystemMessage\n",
        "        existing_summary = \"\"\n",
        "        if self.messages and isinstance(self.messages[0], SystemMessage):\n",
        "            existing_summary = self.messages[0].content\n",
        "\n",
        "        # Create summary prompt\n",
        "        summary_prompt = ChatPromptTemplate.from_messages([\n",
        "     [\n",
        "                SystemMessagePromptTemplate.from_template(\n",
        "                \"Given the existing conversation summary and the new messages, \"\n",
        "                \"generate a new summary of the conversation. Ensure to preserve \"\n",
        "                \"relevant facts and context.\"\n",
        "            ),\n",
        "            HumanMessagePromptTemplate.from_template(\n",
        "                \"Existing conversation summary:\\n{existing_summary}\\n\\n\"\n",
        "                \"New messages:\\n{messages}\"\n",
        "            )\n",
        "     ]\n",
        "        ])\n",
        "\n",
        "        # Generate new summary\n",
        "        response = self.llm.invoke(\n",
        "            summary_prompt.format_messages(\n",
        "                existing_summary=existing_summary,\n",
        "                messages=\"\\n\".join([msg.content for msg in messages])\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Store only the new summary\n",
        "        self.messages = [SystemMessage(content=response.content)]\n",
        "\n",
        "    def clear(self) -> None:\n",
        "        self.messages = []\n"
      ],
      "metadata": {
        "id": "M3cJjvN-I9fR"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_map = {}\n",
        "def get_summary_chat_history(session_id: str, llm: ChatGoogleGenerativeAI) -> ConversationSummaryMessageHistory:\n",
        "    if session_id not in chat_map:\n",
        "        # if session ID doesn't exist, create a new chat history\n",
        "        chat_map[session_id] = ConversationSummaryMessageHistory(llm=llm)\n",
        "    # return the chat history\n",
        "    return chat_map[session_id]"
      ],
      "metadata": {
        "id": "IEvTTWrRMe_u"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain_core.runnables import ConfigurableFieldSpec\n",
        "\n",
        "summary_pipeline = RunnableWithMessageHistory(\n",
        "    pipeline,\n",
        "    get_session_history=get_summary_chat_history,\n",
        "    input_messages_key=\"query\",\n",
        "    history_messages_key=\"history\",\n",
        "    history_factory_config=[\n",
        "\n",
        "            ConfigurableFieldSpec(\n",
        "            id=\"session_id\",\n",
        "            annotation=str,\n",
        "            name=\"Session ID\",\n",
        "            description=\"The session ID to use for the chat history\",\n",
        "            default=\"id_default\",\n",
        "        ),\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"llm\",\n",
        "            annotation=ChatGoogleGenerativeAI,\n",
        "            name=\"LLM\",\n",
        "            description=\"The LLM to use for the conversation summary\",\n",
        "            default=llm,\n",
        "        )\n",
        "    ]\n",
        ",\n",
        ")"
      ],
      "metadata": {
        "id": "3TrQ0qIlNwkS"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary_pipeline.invoke(\n",
        "    {\"query\":\"hello there my name is shanks\"},\n",
        "    config={\"session_id\":\"s_123\",\"llm\":llm}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-2vCxgxSrS7",
        "outputId": "ae3ba327-7e26-459e-d5a4-278cb0186694"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='\"Red-Haired Shanks! Hahahaha! It\\'s been a while, hasn\\'t it? What brings you to my corner of the Grand Line?\"', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--ddd86e77-faf4-4d7e-9291-584759980421-0', usage_metadata={'input_tokens': 33, 'output_tokens': 34, 'total_tokens': 67, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary_pipeline.invoke(\n",
        "    {\"query\":\"hello there my name is black beard\"},\n",
        "    config={\"session_id\":\"s_124\",\"llm\":llm}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nIbF1RBTrwh",
        "outputId": "5266117f-9189-4364-c232-b16a44bf3791"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Zehahaha! Blackbeard, eh? The name suits you, I suppose. What brings you to the Pirate King's attention? Are you looking for a fight, or perhaps a clue to the One Piece? Speak your mind, Teach!\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--50ea1d44-e04c-41e7-bb25-c33e29beec47-0', usage_metadata={'input_tokens': 33, 'output_tokens': 50, 'total_tokens': 83, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary_pipeline.invoke(\n",
        "    {\"query\":\"i heard ace was dead , im sorry i couldnt save him\"},\n",
        "    config={\"session_id\":\"s_123\",\"llm\":llm}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87kZ_qT4UEGU",
        "outputId": "2a2d06ab-72ff-4365-cfd5-50a89cbb0e84"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Shanks, Red-Haired Shanks... to hear you say that... it pains me. Ace... he was a good kid. Stubborn, just like his old man. You did what you could, Shanks. Don't let it weigh you down. The sea is full of sorrow, but also full of adventure. Remember that.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--35c6f1b9-df0b-44be-a2ec-b34bebb9563e-0', usage_metadata={'input_tokens': 65, 'output_tokens': 70, 'total_tokens': 135, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary_pipeline.invoke(\n",
        "    {\"query\":\"im gonna declere a war against world gov if this goes on\"},\n",
        "    config={\"session_id\":\"s_123\",\"llm\":llm}\n",
        ")\n",
        "chat_map[\"s_123\"].messages\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGN4UlcgUNtt",
        "outputId": "0bf3d3a1-5697-4231-8452-485a8891afaf"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content=\"The user, identifying themself as Shanks, tells Gold D. Roger that they heard about Ace's death and apologize for not being able to save him. Roger acknowledges Shanks' sorrow and Ace's character, comforting Shanks by saying he did what he could and reminding him that the sea holds both sorrow and adventure. Shanks then declares his intention to declare war against the World Government. Roger, while understanding Shanks' anger and grief, urges him to reconsider, emphasizing the potential for immense loss of life and global chaos. He reminds Shanks that Ace wouldn't want such a devastating war and suggests finding other ways to honor Ace's memory that don't involve bloodshed. Roger implores Shanks to think carefully about the consequences of his actions.\", additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary_pipeline.invoke(\n",
        "    {\"query\":\"who died\"},\n",
        "    config={\"session_id\":\"s_123\",\"llm\":llm}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmPmFp3wUTDn",
        "outputId": "4660f3eb-cfc7-4155-ef5a-1a6fa3313113"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Shanks, I heard about Ace... I know how much he meant to you, and I'm truly sorry you couldn't save him.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--82c2cac1-1622-4145-a151-e0cde94eb19f-0', usage_metadata={'input_tokens': 164, 'output_tokens': 31, 'total_tokens': 195, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ConversationSummaryBufferMessageHistory(BaseChatMessageHistory, BaseModel):\n",
        "    messages: list[BaseMessage] = Field(default_factory=list)\n",
        "    llm: ChatGoogleGenerativeAI = Field(default_factory=ChatGoogleGenerativeAI)\n",
        "    k: int = Field(default_factory=int)\n",
        "\n",
        "    def __init__(self, llm: ChatGoogleGenerativeAI, k: int):\n",
        "        super().__init__(llm=llm, k=k)\n",
        "\n",
        "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
        "        \"\"\"Add messages to the history, removing any messages beyond\n",
        "        the last `k` messages and summarizing the messages that we\n",
        "        drop.\n",
        "        \"\"\"\n",
        "        existing_summary: SystemMessage | None = None\n",
        "        old_messages: list[BaseMessage] | None = None\n",
        "        # see if we already have a summary message\n",
        "        if len(self.messages) > 0 and isinstance(self.messages[0], SystemMessage):\n",
        "            print(\">> Found existing summary\")\n",
        "            existing_summary = self.messages.pop(0)\n",
        "        # add the new messages to the history\n",
        "        self.messages.extend(messages)\n",
        "        # check if we have too many messages\n",
        "        if len(self.messages) > self.k:\n",
        "            print(\n",
        "                f\">> Found {len(self.messages)} messages, dropping \"\n",
        "                f\"oldest {len(self.messages) - self.k} messages.\")\n",
        "            # pull out the oldest messages...\n",
        "            old_messages = self.messages[:self.k]\n",
        "            # ...and keep only the most recent messages\n",
        "            self.messages = self.messages[-self.k:]\n",
        "        if old_messages is None:\n",
        "            print(\">> No old messages to update summary with\")\n",
        "            # if we have no old_messages, we have nothing to update in summary\n",
        "            return\n",
        "        # construct the summary chat messages\n",
        "        summary_prompt = ChatPromptTemplate.from_messages([\n",
        "            SystemMessagePromptTemplate.from_template(\n",
        "                \"Given the existing conversation summary and the new messages, \"\n",
        "                \"generate a new summary of the conversation. Ensuring to maintain \"\n",
        "                \"as much relevant information as possible.\"\n",
        "            ),\n",
        "            HumanMessagePromptTemplate.from_template(\n",
        "                \"Existing conversation summary:\\n{existing_summary}\\n\\n\"\n",
        "                \"New messages:\\n{old_messages}\"\n",
        "            )\n",
        "        ])\n",
        "        # format the messages and invoke the LLM\n",
        "        new_summary = self.llm.invoke(\n",
        "            summary_prompt.format_messages(\n",
        "                existing_summary=existing_summary,\n",
        "                old_messages=old_messages\n",
        "            )\n",
        "        )\n",
        "        print(f\">> New summary: {new_summary.content}\")\n",
        "        # prepend the new summary to the history\n",
        "        self.messages = [SystemMessage(content=new_summary.content)] + self.messages\n",
        "\n",
        "    def clear(self) -> None:\n",
        "        \"\"\"Clear the history.\"\"\"\n",
        "        self.messages = []"
      ],
      "metadata": {
        "id": "8V4jlnTTYKyD"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_map = {}\n",
        "def get_chat_history(session_id: str, llm: ChatGoogleGenerativeAI, k: int) -> ConversationSummaryBufferMessageHistory:\n",
        "    if session_id not in chat_map:\n",
        "        # if session ID doesn't exist, create a new chat history\n",
        "        chat_map[session_id] = ConversationSummaryBufferMessageHistory(llm=llm, k=k)\n",
        "    # return the chat history\n",
        "    return chat_map[session_id]"
      ],
      "metadata": {
        "id": "dKZzxcxKgSLq"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_with_history = RunnableWithMessageHistory(\n",
        "    pipeline,\n",
        "    get_session_history=get_chat_history,\n",
        "    input_messages_key=\"query\",\n",
        "    history_messages_key=\"history\",\n",
        "    history_factory_config=[\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"session_id\",\n",
        "            annotation=str,\n",
        "            name=\"Session ID\",\n",
        "            description=\"The session ID to use for the chat history\",\n",
        "            default=\"id_default\",\n",
        "        ),\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"llm\",\n",
        "            annotation=ChatGoogleGenerativeAI,\n",
        "            name=\"LLM\",\n",
        "            description=\"The LLM to use for the conversation summary\",\n",
        "            default=llm,\n",
        "        ),\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"k\",\n",
        "            annotation=int,\n",
        "            name=\"k\",\n",
        "            description=\"The number of messages to keep in the history\",\n",
        "            default=4,\n",
        "        )\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "iPB1FsNmggjF"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\":\"hello there\"},\n",
        "    config={\"session_id\":\"s_123\",\"llm\":llm,\"k\":4}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIkKKlVlg2Yw",
        "outputId": "27bddff6-fdb7-4a42-e48c-40f19c6d78c2"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">> No old messages to update summary with\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Yo ho ho! Well met, matey! Gold D. Roger at your service! What brings ye to these waters? Are ye seekin' adventure, treasure, or perhaps just a good yarn? Tell me, what's on yer mind?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--ba66eeb7-8af9-4935-89a0-593bea4cc8a1-0', usage_metadata={'input_tokens': 15, 'output_tokens': 52, 'total_tokens': 67, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\":\"My name is shanks\"},\n",
        "    config={\"session_id\":\"s_123\",\"llm\":llm,\"k\":4}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQH6jGs3hHoA",
        "outputId": "5366adb9-6685-4394-ae45-ac47405f5fdc"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">> No old messages to update summary with\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Shanks, eh? A fine name, a name I've heard whispered on the winds. You've got a fire in your eyes, lad. A fire I recognize. What brings you to me, Shanks? Are you looking for a crew, a challenge, or perhaps a glimpse of the future? Tell me, what's your ambition? What do you seek on the Grand Line?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--26bb4e24-b53f-4713-947f-1d9e8726c441-0', usage_metadata={'input_tokens': 71, 'output_tokens': 82, 'total_tokens': 153, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_map[\"s_123\"].messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqJtFdJRjiiX",
        "outputId": "adf82bb6-6fce-4226-ea20-2cc43c051791"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='hello there', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"Yo ho ho! Well met, matey! Gold D. Roger at your service! What brings ye to these waters? Are ye seekin' adventure, treasure, or perhaps just a good yarn? Tell me, what's on yer mind?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--ba66eeb7-8af9-4935-89a0-593bea4cc8a1-0', usage_metadata={'input_tokens': 15, 'output_tokens': 52, 'total_tokens': 67, 'input_token_details': {'cache_read': 0}}),\n",
              " HumanMessage(content='My name is shanks', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"Shanks, eh? A fine name, a name I've heard whispered on the winds. You've got a fire in your eyes, lad. A fire I recognize. What brings you to me, Shanks? Are you looking for a crew, a challenge, or perhaps a glimpse of the future? Tell me, what's your ambition? What do you seek on the Grand Line?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--26bb4e24-b53f-4713-947f-1d9e8726c441-0', usage_metadata={'input_tokens': 71, 'output_tokens': 82, 'total_tokens': 153, 'input_token_details': {'cache_read': 0}})]"
            ]
          },
          "metadata": {},
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\":\"i went to black beard once\"},\n",
        "    config={\"session_id\":\"s_123\",\"llm\":llm,\"k\":4}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDUdpucBjq6A",
        "outputId": "57422628-fdf3-4dee-bd6a-b0c6a0688fdc"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">> Found 6 messages, dropping oldest 2 messages.\n",
            ">> New summary: The conversation begins with a greeting. Gold D. Roger responds in a pirate persona, asking the user what they seek. The user then identifies themself as Shanks, prompting Roger to inquire about Shanks' ambitions and purpose on the Grand Line.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Blackbeard, you say? Teach, that sly dog. He's got a darkness about him, a hunger that never seems to be satisfied. What was your business with him, Shanks? Did you see the glint of ambition in his eyes? Did you sense the danger he carries within him? Tell me, what did you learn from your encounter with Blackbeard? It's a name I won't soon forget, and any knowledge of his movements is valuable indeed.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--7e6b36e6-a739-4edb-8d62-20db0882fa85-0', usage_metadata={'input_tokens': 158, 'output_tokens': 98, 'total_tokens': 256, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 162
        }
      ]
    }
  ]
}
